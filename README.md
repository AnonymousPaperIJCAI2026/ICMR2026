<div align="left"> 
<h1> ğŸ“Œ TACO </h1>
<h3>TACO: Token Calibration with Prompt Anchors for Cross-Domain Zero/Few-Shot Anomaly Detection and Localization</h3>
</div>


<div align="center"> <img src="figs/1.png" width="60%"> </div>

<div align="justify">

## â­ Abstract 
Industrial anomaly detection and localization increasingly rely on large vision--language models (VLMs) for their strong priors and promising zero-shot transfer. However, directly deploying VLMs across unseen industrial domains remains challenging due to (i) semantic inconsistency caused by prompt sensitivity and domain shifts, and (ii) insufficient, task-mismatched local evidence for reliable pixel-level localization. To address these issues, we propose TACO, a parameter-efficient framework that couples prompt anchors with token calibration for zero- and few-shot cross-domain anomaly detection and localization. TACO learns stable normal/ abnormal textual anchors from a prompt ensemble (TWPP), restores anomaly-sensitive local cues via lightweight token adapters with enhanced aggregation (DTA-EA), and performs low-rank cross-modal calibration that injects anchor semantics into visual tokens to reduce representational drift and improve token separability (PTC). The framework further supports few-shot target adaptation by building an inference-time memory bank from a few normal samples. Extensive experiments on diverse 2D benchmarks (VisA, BTAD, DAGM, DTD-Synthetic) and the 3D MVTec-3D dataset demonstrate superior transferability. In zero-shot 2D cross-domain evaluation, TACO establishes a new state-of-the-art with 94.2\% Image-AUROC and 95.8\% AP, surpassing the strongest prior baseline by +1.4\%. For pixel-level localization, it achieves 95.7\% Pixel-AUROC and 87.4\% PRO. Furthermore, our few-shot adaptation on MVTec-3D significantly outperforms existing methods, achieving 81.8\% AUROC in the 1-shot setting---surpassing the previous state-of-the-art by +2.6\%.
</div>

ğŸ“´**Keywords**: Cross-Domain, Zero/Few-Shot, Anomaly Detection and Segmentation, Large Vision Language Model

<div align="center"> <img src="figs/2.png " width="100%"> </div>


## ğŸš€ Get Started

âš™ï¸ Environment
- python >= 3.8.5
- pytorch >= 1.10.0
- torchvision >= 0.11.1
- numpy >= 1.19.2
- scipy >= 1.5.2
- kornia >= 0.6.1
- pandas >= 1.1.3
- opencv-python >= 4.5.4
- pillow
- tqdm
- ftfy
- regex

### Device
Single NVIDIA A40 GPU

## ğŸ“¦ Pretrained model
- CLIP: ##################################################################

    ğŸ‘‰ Download and put it under `CLIP/ckpt` folder



## ğŸ¥ğŸ­ Medical and Industrial Anomaly Detection Benchmark(2D\3D)

1. We will provide the pre-processed benchmark. Please download the following dataset

    

2. Place it within the master directory `data` and unzip the dataset.

    ```
    
    ```


## ğŸ“‚ File Structure
After the preparation work, the whole project should have the following structure:

```
code
â”œâ”€ ckpt
â”‚  â””â”€ zero-shot
â”œâ”€ CLIP
â”‚  â”œâ”€ bpe_simple_vocab_16e6.txt.gz
â”‚  â”œâ”€ ckpt
â”‚  â”‚  â””â”€ ViT-L-14-336px.pt
â”‚  â”œâ”€ clip.py
â”‚  â”œâ”€ model.py
â”‚  â”œâ”€ models.py
â”‚  â”œâ”€ model_configs
â”‚  â”‚  â””â”€ ViT-L-14-336.json
â”‚  â”œâ”€ modified_resnet.py
â”‚  â”œâ”€ openai.py
â”‚  â”œâ”€ tokenizer.py
â”‚  â””â”€ transformer.py
â”œâ”€ data
â”‚  â”œâ”€ Mvtec3D
â”‚  â”‚  â”œâ”€ valid
â”‚  â”‚  â””â”€ test
â”‚  â”œâ”€ BTAD
â”‚  â”‚  â”œâ”€ valid
â”‚  â”‚  â””â”€ test
â”‚  â”œâ”€ Mvtec
â”‚  â”‚  â”œâ”€ valid
â”‚  â”‚  â””â”€ test
â”‚  â”œâ”€ ...
â”‚  â””â”€ Visa
â”‚     â”œâ”€ valid
â”‚     â””â”€ test
â”œâ”€ dataset
â”‚  â”œâ”€ fewshot_seed
â”‚  â”‚  â””â”€ Mvtec3D
â”‚  â”œâ”€ medical_few.py
â”‚  â””â”€ medical_zero.py
â”œâ”€ loss.py
â”œâ”€ readme.md
â”œâ”€ train_few.py
â”œâ”€ train_zero.py
â”œâ”€ test.py
â””â”€ utils.py

```


## âš¡ Quick Start

`python test.py`

For example, to test on the Visa , simply run:

`###########################`

### Training

`python train_zero.py`




## ğŸ–¼ï¸ Visualization
<center><img src="images/3.png "width="70%"></center>
<center><img src="images/4.png "width="70%"></center>
<center><img src="images/5.png "width="70%"></center>

## ğŸ™ Acknowledgement
We borrow some codes from [OpenCLIP](https://github.com/mlfoundations/open_clip), and [April-GAN](https://github.com/ByChelsea/VAND-APRIL-GAN).

## ğŸ“¬ Contact

If you have any problem with this code, please feel free to contact **** and ****.

